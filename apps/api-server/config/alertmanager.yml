# AlertManager Configuration for AI Page Builder
# Sprint 4: Alert Routing and Notification
#
# Usage:
# docker run -d -p 9093:9093 -v $(pwd)/alertmanager.yml:/etc/alertmanager/alertmanager.yml prom/alertmanager

global:
  # Default notification settings
  resolve_timeout: 5m
  # Slack webhook URL (configure in environment)
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# Templates for notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for organizing alerts
route:
  # Default receiver for all alerts
  receiver: 'default'

  # Group alerts by these labels
  group_by: ['alertname', 'component']

  # Wait time before sending first notification
  group_wait: 10s

  # Wait time before sending notifications about new alerts in an existing group
  group_interval: 10s

  # Wait time before re-sending an alert
  repeat_interval: 3h

  # Child routes (more specific routing)
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'slack-critical'
      group_wait: 5s
      repeat_interval: 30m
      continue: true # Also send to default receiver

    # Warning alerts - batched notification
    - match:
        severity: warning
      receiver: 'slack-warnings'
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h

    # Info alerts - daily digest
    - match:
        severity: info
      receiver: 'slack-info'
      group_wait: 1m
      group_interval: 1h
      repeat_interval: 24h

    # Component-specific routing
    - match:
        component: ai-worker
      receiver: 'slack-ai-worker'
      group_by: ['alertname', 'provider', 'model']

    - match:
        component: queue
      receiver: 'slack-queue'

    - match:
        component: cost
      receiver: 'slack-cost'

# Inhibition rules (suppress certain alerts based on others)
inhibit_rules:
  # If critical failure rate alert is firing, don't send warning failure rate alert
  - source_match:
      alertname: 'CriticalAIJobFailureRate'
    target_match:
      alertname: 'HighAIJobFailureRate'
    equal: ['component']

  # If queue is severely congested, don't send regular congestion alert
  - source_match:
      alertname: 'AIQueueSeverelyCongested'
    target_match:
      alertname: 'AIQueueCongestion'
    equal: ['component']

# Alert receivers (notification destinations)
receivers:
  # Default receiver - logs only
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:3000/api/alerts/webhook'
        send_resolved: true

  # Critical alerts - Slack with @channel mention
  - name: 'slack-critical'
    slack_configs:
      - channel: '#ai-alerts-critical'
        username: 'AlertManager'
        icon_emoji: ':rotating_light:'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Component:* {{ .Labels.component }}
          {{ end }}
        send_resolved: true
        actions:
          - type: button
            text: 'View Grafana Dashboard'
            url: 'http://localhost:3001/d/ai-page-builder-ops'
          - type: button
            text: 'View Prometheus'
            url: 'http://localhost:9090/alerts'

  # Warning alerts - Slack
  - name: 'slack-warnings'
    slack_configs:
      - channel: '#ai-alerts'
        username: 'AlertManager'
        icon_emoji: ':warning:'
        title: '⚠️ {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
        send_resolved: true

  # Info alerts - Slack (no @channel)
  - name: 'slack-info'
    slack_configs:
      - channel: '#ai-alerts-info'
        username: 'AlertManager'
        icon_emoji: ':information_source:'
        title: 'ℹ️ {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.summary }}
          {{ end }}
        send_resolved: false

  # AI worker specific alerts
  - name: 'slack-ai-worker'
    slack_configs:
      - channel: '#ai-worker-alerts'
        username: 'AI Worker Monitor'
        icon_emoji: ':robot_face:'
        title: '{{ .GroupLabels.alertname }} - {{ .GroupLabels.provider }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Provider:* {{ .Labels.provider }}
          *Model:* {{ .Labels.model }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  # Queue alerts
  - name: 'slack-queue'
    slack_configs:
      - channel: '#ai-queue-alerts'
        username: 'Queue Monitor'
        icon_emoji: ':arrows_counterclockwise:'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  # Cost alerts - for finance team
  - name: 'slack-cost'
    slack_configs:
      - channel: '#ai-cost-alerts'
        username: 'Cost Monitor'
        icon_emoji: ':moneybag:'
        title: 'Cost Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          {{ .Annotations.description }}
          {{ end }}
        send_resolved: false

# Additional notification channels (email example - commented out)
# - name: 'email-ops'
#   email_configs:
#     - to: 'ops-team@example.com'
#       from: 'alertmanager@example.com'
#       smarthost: 'smtp.gmail.com:587'
#       auth_username: 'alerts@example.com'
#       auth_password: '${EMAIL_PASSWORD}'
#       headers:
#         Subject: '[AI Page Builder] {{ .GroupLabels.alertname }}'
